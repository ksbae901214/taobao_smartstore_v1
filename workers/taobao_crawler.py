import os
import json
import time
import asyncio
import re
from playwright.async_api import async_playwright
from database import get_db_connection
import redis
from loguru import logger

# Redis ì—°ê²°
redis_client = redis.Redis(
    host=os.getenv('REDIS_HOST', 'redis'),
    port=int(os.getenv('REDIS_PORT', 6379)),
    decode_responses=True
)

logger.info("ğŸ¤– íƒ€ì˜¤ë°”ì˜¤ í¬ë¡¤ëŸ¬ ì›Œì»¤ ì‹œì‘...")

async def crawl_taobao_product(url):
    """
    íƒ€ì˜¤ë°”ì˜¤ ìƒí’ˆ í˜ì´ì§€ í¬ë¡¤ë§
    """
    logger.info(f"ğŸ” í¬ë¡¤ë§ ì‹œì‘: {url}")
    
    result = {
        'url': url,
        'success': False,
        'product_id': None,
        'title_cn': None,
        'price_cny': None,
        'thumbnails': [],
        'detail_images': [],
        'options': [],
        'stock': 0,
        'description_cn': None
    }
    
    try:
        # ìƒí’ˆ ID ì¶”ì¶œ
        product_id_match = re.search(r'id=(\d+)', url)
        if product_id_match:
            result['product_id'] = product_id_match.group(1)
        
        async with async_playwright() as p:
            # ë¸Œë¼ìš°ì € ì‹œì‘
            browser = await p.chromium.launch(
                headless=True,
                args=[
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-blink-features=AutomationControlled'
                ]
            )
            
            context = await browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            )
            
            page = await context.new_page()
            
            # íƒ€ì˜¤ë°”ì˜¤ í˜ì´ì§€ ì ‘ì†
            logger.info(f"ğŸ“„ í˜ì´ì§€ ì ‘ì† ì¤‘...")
            await page.goto(url, wait_until='domcontentloaded', timeout=30000)
            await page.wait_for_timeout(3000)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            
            # ìƒí’ˆëª… ì¶”ì¶œ
            try:
                title_selectors = [
                    'h1[class*="title"]',
                    'div[class*="tb-detail-hd"] h1',
                    'div[class*="ItemTitle"] h1',
                    'h3[class*="title"]',
                    '.tb-main-title'
                ]
                
                for selector in title_selectors:
                    title_element = await page.query_selector(selector)
                    if title_element:
                        result['title_cn'] = await title_element.inner_text()
                        result['title_cn'] = result['title_cn'].strip()
                        logger.info(f"âœ… ìƒí’ˆëª…: {result['title_cn'][:50]}...")
                        break
            except Exception as e:
                logger.error(f"ìƒí’ˆëª… ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # ê°€ê²© ì¶”ì¶œ
            try:
                price_selectors = [
                    'span[class*="priceText"]',
                    'em[class*="tb-rmb-num"]',
                    'span[class*="price"]',
                    '.tb-price',
                    'strong[class*="price"]'
                ]
                
                for selector in price_selectors:
                    price_element = await page.query_selector(selector)
                    if price_element:
                        price_text = await price_element.inner_text()
                        # ìˆ«ìë§Œ ì¶”ì¶œ
                        price_match = re.search(r'[\d.]+', price_text)
                        if price_match:
                            result['price_cny'] = float(price_match.group())
                            logger.info(f"âœ… ê°€ê²©: Â¥{result['price_cny']}")
                            break
            except Exception as e:
                logger.error(f"ê°€ê²© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # ì¸ë„¤ì¼ ì´ë¯¸ì§€ ì¶”ì¶œ
            try:
                thumbnail_selectors = [
                    'ul[class*="tb-thumb"] img',
                    'div[class*="gallery"] img',
                    'ul[id*="J_UlThumb"] img'
                ]
                
                for selector in thumbnail_selectors:
                    thumbnails = await page.query_selector_all(selector)
                    if thumbnails:
                        for thumb in thumbnails[:5]:  # ìµœëŒ€ 5ê°œ
                            img_url = await thumb.get_attribute('src')
                            if not img_url:
                                img_url = await thumb.get_attribute('data-src')
                            if img_url:
                                # HTTPë¡œ ì‹œì‘í•˜ì§€ ì•Šìœ¼ë©´ ì¶”ê°€
                                if img_url.startswith('//'):
                                    img_url = 'https:' + img_url
                                result['thumbnails'].append(img_url)
                        logger.info(f"âœ… ì¸ë„¤ì¼: {len(result['thumbnails'])}ê°œ")
                        break
            except Exception as e:
                logger.error(f"ì¸ë„¤ì¼ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # ìƒì„¸ ì´ë¯¸ì§€ ì¶”ì¶œ
            try:
                detail_selectors = [
                    'div[class*="detail"] img',
                    'div[id*="description"] img',
                    'div[class*="desc"] img'
                ]
                
                for selector in detail_selectors:
                    detail_imgs = await page.query_selector_all(selector)
                    if detail_imgs:
                        for img in detail_imgs[:10]:  # ìµœëŒ€ 10ê°œ
                            img_url = await img.get_attribute('src')
                            if not img_url:
                                img_url = await img.get_attribute('data-src')
                            if img_url and img_url not in result['detail_images']:
                                if img_url.startswith('//'):
                                    img_url = 'https:' + img_url
                                result['detail_images'].append(img_url)
                        logger.info(f"âœ… ìƒì„¸ì´ë¯¸ì§€: {len(result['detail_images'])}ê°œ")
                        break
            except Exception as e:
                logger.error(f"ìƒì„¸ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # ì¬ê³  ì¶”ì¶œ (ì¶”ì •)
            try:
                result['stock'] = 999  # ê¸°ë³¸ê°’
            except Exception as e:
                logger.error(f"ì¬ê³  ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            await browser.close()
            result['success'] = True
            logger.info(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
            
    except Exception as e:
        logger.error(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        result['error'] = str(e)
    
    return result

async def process_crawl_queue():
    """
    Redis íì—ì„œ í¬ë¡¤ë§ ì‘ì—… ì²˜ë¦¬
    """
    logger.info("â³ í¬ë¡¤ë§ í ëª¨ë‹ˆí„°ë§ ì‹œì‘...")
    
    while True:
        try:
            # Redisì—ì„œ ì‘ì—… ê°€ì ¸ì˜¤ê¸°
            job_data = redis_client.brpop('crawl_queue', timeout=5)
            
            if job_data:
                queue_name, job_json = job_data
                job = json.loads(job_json)
                
                url = job.get('url')
                product_id = job.get('product_id')
                
                logger.info(f"ğŸ“¦ ìƒˆ ì‘ì—…: {product_id}")
                
                # í¬ë¡¤ë§ ì‹¤í–‰
                result = await crawl_taobao_product(url)
                
                # ê²°ê³¼ë¥¼ Redisì— ì €ì¥ (ê²°ê³¼ í)
                result_key = f"crawl_result:{product_id}"
                redis_client.setex(
                    result_key,
                    3600,  # 1ì‹œê°„ í›„ ë§Œë£Œ
                    json.dumps(result, ensure_ascii=False)
                )
                
                logger.info(f"âœ… ê²°ê³¼ ì €ì¥: {result_key}")
                
                # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (ì„ íƒì‚¬í•­)
                try:
                    conn = get_db_connection()
                    cursor = conn.cursor()
                    
                    cursor.execute("""
                        INSERT INTO products (
                            taobao_product_id, taobao_url, status, 
                            title_cn, price_cny, stock_quantity, crawled_at
                        ) VALUES (%s, %s, %s, %s, %s, %s, NOW())
                        ON CONFLICT (taobao_product_id) 
                        DO UPDATE SET 
                            title_cn = EXCLUDED.title_cn,
                            price_cny = EXCLUDED.price_cny,
                            stock_quantity = EXCLUDED.stock_quantity,
                            crawled_at = NOW()
                        RETURNING id
                    """, (
                        product_id,
                        url,
                        'scraped',
                        result.get('title_cn'),
                        result.get('price_cny'),
                        result.get('stock', 0)
                    ))
                    
                    product_db_id = cursor.fetchone()['id']
                    
                    # ì´ë¯¸ì§€ ì €ì¥
                    for idx, img_url in enumerate(result.get('thumbnails', [])):
                        cursor.execute("""
                            INSERT INTO product_images (
                                product_id, image_type, original_url, sort_order
                            ) VALUES (%s, %s, %s, %s)
                            ON CONFLICT DO NOTHING
                        """, (product_db_id, 'thumbnail', img_url, idx))
                    
                    for idx, img_url in enumerate(result.get('detail_images', [])):
                        cursor.execute("""
                            INSERT INTO product_images (
                                product_id, image_type, original_url, sort_order
                            ) VALUES (%s, %s, %s, %s)
                            ON CONFLICT DO NOTHING
                        """, (product_db_id, 'detail', img_url, idx))
                    
                    conn.commit()
                    cursor.close()
                    conn.close()
                    
                    logger.info(f"âœ… DB ì €ì¥ ì™„ë£Œ: product_id={product_db_id}")
                    
                except Exception as e:
                    logger.error(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")
                
        except Exception as e:
            logger.error(f"âŒ í ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            await asyncio.sleep(5)

if __name__ == '__main__':
    # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸
    from database import test_connection
    test_connection()
    
    # í¬ë¡¤ë§ í ì²˜ë¦¬ ì‹œì‘
    asyncio.run(process_crawl_queue())
